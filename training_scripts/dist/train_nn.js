"use strict";
/**
 * @file Main Training Script (Node.js, Memory Optimized)
 * This script trains the dual-head ResNet model using data generated by self-play.
 * It processes data in chunks to keep memory usage low.
 */
var __createBinding = (this && this.__createBinding) || (Object.create ? (function(o, m, k, k2) {
    if (k2 === undefined) k2 = k;
    var desc = Object.getOwnPropertyDescriptor(m, k);
    if (!desc || ("get" in desc ? !m.__esModule : desc.writable || desc.configurable)) {
      desc = { enumerable: true, get: function() { return m[k]; } };
    }
    Object.defineProperty(o, k2, desc);
}) : (function(o, m, k, k2) {
    if (k2 === undefined) k2 = k;
    o[k2] = m[k];
}));
var __setModuleDefault = (this && this.__setModuleDefault) || (Object.create ? (function(o, v) {
    Object.defineProperty(o, "default", { enumerable: true, value: v });
}) : function(o, v) {
    o["default"] = v;
});
var __importStar = (this && this.__importStar) || (function () {
    var ownKeys = function(o) {
        ownKeys = Object.getOwnPropertyNames || function (o) {
            var ar = [];
            for (var k in o) if (Object.prototype.hasOwnProperty.call(o, k)) ar[ar.length] = k;
            return ar;
        };
        return ownKeys(o);
    };
    return function (mod) {
        if (mod && mod.__esModule) return mod;
        var result = {};
        if (mod != null) for (var k = ownKeys(mod), i = 0; i < k.length; i++) if (k[i] !== "default") __createBinding(result, mod, k[i]);
        __setModuleDefault(result, mod);
        return result;
    };
})();
Object.defineProperty(exports, "__esModule", { value: true });
const tf = __importStar(require("@tensorflow/tfjs-node-gpu"));
const node_fs_1 = require("node:fs");
const path = __importStar(require("path"));
const model_1 = require("./src/model");
// --- Configuration ---
const DATA_FILE_PATH = './training_data.jsonl';
const MODEL_SAVE_PATH = './gomoku_model';
const BOARD_SIZE = 19;
// --- Training Hyperparameters ---
const EPOCHS = 10;
const CHUNK_SIZE = 8192; // Number of original samples to process at a time
const BATCH_SIZE = 128;
// --- Data Processing and Augmentation ---
function parseLinesToSamples(lines) {
    return lines.filter(line => line).map(line => JSON.parse(line));
}
function augmentAndConvertToTensors(samples) {
    return tf.tidy(() => {
        const augmentedStates = [];
        const augmentedPolicies = [];
        const augmentedValues = [];
        for (const sample of samples) {
            const symmetries = getSymmetries(sample.state, sample.policy);
            for (const sym of symmetries) {
                const player = sample.player || 'black';
                const opponent = player === 'black' ? 'white' : 'black';
                const playerChannel = Array(BOARD_SIZE).fill(0).map(() => Array(BOARD_SIZE).fill(0));
                const opponentChannel = Array(BOARD_SIZE).fill(0).map(() => Array(BOARD_SIZE).fill(0));
                for (let r = 0; r < BOARD_SIZE; r++) {
                    for (let c = 0; c < BOARD_SIZE; c++) {
                        if (sym.state[r][c] === player)
                            playerChannel[r][c] = 1;
                        else if (sym.state[r][c] === opponent)
                            opponentChannel[r][c] = 1;
                    }
                }
                const colorChannel = Array(BOARD_SIZE).fill(0).map(() => Array(BOARD_SIZE).fill(player === 'black' ? 1 : 0));
                const stacked = tf.stack([tf.tensor2d(playerChannel), tf.tensor2d(opponentChannel), tf.tensor2d(colorChannel)], 2);
                augmentedStates.push(stacked.expandDims(0));
                augmentedPolicies.push(tf.tensor2d([sym.policy]));
                augmentedValues.push([sample.value]);
            }
        }
        return {
            xs: tf.concat(augmentedStates),
            ys: {
                policy: tf.concat(augmentedPolicies),
                value: tf.tensor2d(augmentedValues)
            }
        };
    });
}
function getSymmetries(state, policy) {
    const symmetries = [];
    let currentBoard = state.map(row => [...row]);
    let currentPolicy = [...policy];
    for (let i = 0; i < 4; i++) {
        symmetries.push({ state: currentBoard, policy: currentPolicy });
        symmetries.push({ state: flipBoard(currentBoard), policy: flipPolicy(currentPolicy) });
        currentBoard = rotateBoard(currentBoard);
        currentPolicy = rotatePolicy(currentPolicy);
    }
    return symmetries;
}
function rotateBoard(board) {
    const newBoard = Array(BOARD_SIZE).fill(null).map(() => Array(BOARD_SIZE).fill(null));
    for (let r = 0; r < BOARD_SIZE; r++) {
        for (let c = 0; c < BOARD_SIZE; c++) {
            newBoard[c][BOARD_SIZE - 1 - r] = board[r][c];
        }
    }
    return newBoard;
}
function flipBoard(board) {
    return board.map(row => row.slice().reverse());
}
function rotatePolicy(policy) {
    const newPolicy = Array(BOARD_SIZE * BOARD_SIZE).fill(0);
    for (let r = 0; r < BOARD_SIZE; r++) {
        for (let c = 0; c < BOARD_SIZE; c++) {
            newPolicy[c * BOARD_SIZE + (BOARD_SIZE - 1 - r)] = policy[r * BOARD_SIZE + c];
        }
    }
    return newPolicy;
}
function flipPolicy(policy) {
    const newPolicy = Array(BOARD_SIZE * BOARD_SIZE).fill(0);
    for (let r = 0; r < BOARD_SIZE; r++) {
        for (let c = 0; c < BOARD_SIZE; c++) {
            newPolicy[r * BOARD_SIZE + (BOARD_SIZE - 1 - c)] = policy[r * BOARD_SIZE + c];
        }
    }
    return newPolicy;
}
// --- Main Training Logic ---
async function train() {
    console.log(`Loading training data lines from ${DATA_FILE_PATH}...`);
    let allLines;
    try {
        const fileContent = (0, node_fs_1.readFileSync)(DATA_FILE_PATH, 'utf-8');
        allLines = fileContent.trim().split('\n');
        if (allLines.length === 0 || (allLines.length === 1 && !allLines[0])) {
            console.error('Training data file is empty.');
            return;
        }
        console.log(`Loaded ${allLines.length} data lines.`);
    }
    catch (error) {
        console.error(`Error reading training data file: ${error}`);
        return;
    }
    let model;
    try {
        model = await tf.loadLayersModel(`file://${MODEL_SAVE_PATH}/model.json`);
        console.log('Existing model loaded.');
    }
    catch (error) {
        console.log('No existing model found. Creating a new model...');
        model = (0, model_1.createDualResNetModel)();
    }
    model.compile({
        optimizer: tf.train.adam(),
        loss: { 'policy': 'categoricalCrossentropy', 'value': 'meanSquaredError' },
        metrics: { 'policy': 'accuracy', 'value': tf.metrics.meanAbsoluteError }
    });
    const NUM_CHUNKS = Math.ceil(allLines.length / CHUNK_SIZE);
    for (let epoch = 0; epoch < EPOCHS; epoch++) {
        console.log(`\n--- Epoch ${epoch + 1} / ${EPOCHS} ---\
`);
        tf.util.shuffle(allLines);
        for (let i = 0; i < NUM_CHUNKS; i++) {
            const start = i * CHUNK_SIZE;
            const end = Math.min(start + CHUNK_SIZE, allLines.length);
            const lineChunk = allLines.slice(start, end);
            console.log(`\nProcessing chunk ${i + 1}/${NUM_CHUNKS} (lines ${start}-${end})
`);
            const samples = parseLinesToSamples(lineChunk);
            if (samples.length === 0)
                continue;
            const { xs, ys } = augmentAndConvertToTensors(samples);
            await model.fit(xs, ys, {
                batchSize: BATCH_SIZE,
                epochs: 1,
                shuffle: true,
            });
            xs.dispose();
            ys.policy.dispose();
            ys.value.dispose();
            console.log(`Memory freed for chunk ${i + 1}. Num Tensors: ${tf.memory().numTensors}`);
        }
    }
    console.log('\nModel training finished.');
    console.log(`Saving model to ${MODEL_SAVE_PATH}...`);
    await model.save(`file://${path.resolve(MODEL_SAVE_PATH)}`);
    console.log('Model saved successfully.');
}
train().catch(console.error);
